{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hugging face NLP tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxmsBf2Ok7gojzW0cmhtXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arjun-krish-10/NLP/blob/main/Hugging_face_NLP_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNGPoQ0mBDLl",
        "outputId": "b60e3d9d-1879-4385-cf72-50bed67054f4"
      },
      "source": [
        " !pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gKEkIOgBMzl"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YWQ0I3Rb5OG"
      },
      "source": [
        "#Zero shot learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD_G-zsCzlc"
      },
      "source": [
        "from transformers import pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FerSLVDwZNab"
      },
      "source": [
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"valhalla/distilbart-mnli-12-9\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-lV2m0aBVt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd50280-64ae-45da-f628-8d5dcf583194"
      },
      "source": [
        "sequence_to_classify = \"Roger Fedderer won the World championship\"\n",
        "candidate_labels = ['Football','Cricket','Tennis','Chess']\n",
        "hypothesis_template = 'The sport being talked is {}'\n",
        "classifier(sequence_to_classify, candidate_labels,hypothesis_template=hypothesis_template)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': ['Tennis', 'Chess', 'Football', 'Cricket'],\n",
              " 'scores': [0.5370711088180542,\n",
              "  0.28500232100486755,\n",
              "  0.10809198021888733,\n",
              "  0.06983468681573868],\n",
              " 'sequence': 'Roger Fedderer won the World championship'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm2UOL7_UsH_"
      },
      "source": [
        "# !pip install transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import  TFAutoModelForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "from transformers import  TFAutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelWithLMHead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2HoMQoSkoZU"
      },
      "source": [
        "\n",
        "tokenizer_bart_large = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
        "model_zero_shot = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBLmvf3-n26O",
        "outputId": "c0e862e9-18c4-450b-fabb-50981fb05918"
      },
      "source": [
        "# pose sequence as a NLI premise and label (politics) as a hypothesis\n",
        "# https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
        "premise = 'Sachin Tendulkar is playing the world championship'\n",
        "hypothesis = 'This text is about chess.'\n",
        "\n",
        "# run through model pre-trained on MNLI\n",
        "input_ids = tokenizer_bart_large.encode(premise, hypothesis, return_tensors='pt')\n",
        "logits = model_zero_shot(input_ids)[0]\n",
        "\n",
        "# we throw away \"neutral\" (dim 1) and take the probability of\n",
        "# \"entailment\" (2) as the probability of the label being true \n",
        "entail_contradiction_logits = logits[:,[0,1,2]]\n",
        "probs = entail_contradiction_logits.softmax(dim=1)\n",
        "true_prob = probs[:,1].item() * 100\n",
        "print(f'Probability that the label is true: {true_prob:0.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability that the label is true: 0.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI9uTUcZcJZ5"
      },
      "source": [
        "#Question & Answering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQGYXcGgDBG0",
        "outputId": "aeff8db8-e1e3-49e6-84fb-1f9b03ba0079"
      },
      "source": [
        "\n",
        "\n",
        "tokenizer_qa = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "model_qa = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-uncased-distilled-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro1hiCaMVrUZ",
        "outputId": "e0f52189-ab2d-41ea-9c92-5322780a9cf2"
      },
      "source": [
        "text = r\"\"\"\n",
        "Ip Man is a Kung Fu movie about the legendary martial arts teacher of the same name. It is rated a staggering 8 out of 10 on IMDb and considered a cult classic among fans.\n",
        "\n",
        "The movie is almost two hours long, but if you skim through it, you‚Äôll notice: There‚Äôs not a lot of fighting. Isn‚Äôt that what Kung Fu movies are about? Apparently not.\n",
        "\n",
        "You‚Äôll see the master having tea, helping his friends, and struggling with everyday life. You‚Äôll see him muse about politics, about war, and about philosophy. You‚Äôll even see Ip Man spending time with his family and training with ‚Äú the wooden man ,‚Äù a tool he invented.\n",
        "\n",
        "Why do people love this movie so much if, as it turns out, there are only three major fight scenes? They love it because each fight means something . That‚Äôs what Kung Fu is really about: Learning to use martial arts when it matters .\n",
        "\n",
        "Ip Man perfectly represents this ideal, and that‚Äôs why he‚Äôs an admirable character. He doesn‚Äôt fight just to fight. He wants to maintain peace among his community. Only if the fighting serves a higher goal does he break out his fists.\n",
        "\n",
        "In the first fight, Ip Man must defend his home against an intruder. In the second, he avenges a friend to send a message. In the third, he makes an example of the leader of the Japanese, occupying forces.\n",
        "\n",
        "Family, loyalty, and culture. Those are the themes behind Ip Man‚Äôs fights, and they‚Äôre much bigger than just himself. That‚Äôs why it‚Äôs an honor to watch him fight and easy to root for him when he does.\n",
        "\n",
        "There are a lot of lessons in the movie about values, about civility, and about the true philosophy of Kung Fu, but the main one may be: ‚ÄúDon‚Äôt fight when it doesn‚Äôt matter.‚Äù Focus your energy on the biggest obstacles so you may overcome them when they appear.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "\"What is the main lesson in the article?\"\n",
        "# \"What does ü§ó Transformers provide?\",\n",
        "# \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  inputs = tokenizer_qa(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "\n",
        "  outputs = model_qa(inputs)\n",
        "  # print(outputs)\n",
        "  answer_start_scores = outputs.start_logits\n",
        "  answer_end_scores = outputs.end_logits\n",
        "  # print(answer_start_scores,answer_end_scores)\n",
        "  answer_start = tf.argmax(\n",
        "      answer_start_scores, axis=1\n",
        "  ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n",
        "  \n",
        "  answer_end = (\n",
        "      tf.argmax(answer_end_scores, axis=1) + 1\n",
        "  ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n",
        "  print(answer_start,answer_end)\n",
        "\n",
        "  answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "390 400\n",
            "Question: What is the main lesson in the article?\n",
            "Answer: don ‚Äô t fight when it doesn ‚Äô t matter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5raaExknDSNk"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqDq5UHCcv92"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA4re8FlDexb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1840c5f1-0f4f-4954-d66f-5f026f2cc09b"
      },
      "source": [
        "  \n",
        "tokenizer_clf = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "\n",
        "model_clf = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "# model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-large-mnli\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4F7so5XDqin"
      },
      "source": [
        "text = 'It was a good movie'\n",
        "tok = tokenizer_clf.encode_plus(text,add_special_tokens=True,return_tensors='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmFurAbSD4K8"
      },
      "source": [
        "# ip = np.array(tok['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMnZVQ5lEWCV"
      },
      "source": [
        "output = model_clf(tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TyvuB5SEsng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a890b90f-c304-4e9d-cb50-49254e9f3fde"
      },
      "source": [
        "output.logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-4.2403626,  4.6366396]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDQ4C2Mspp-r",
        "outputId": "c830a363-46ec-40aa-8f49-bf37fbf83824"
      },
      "source": [
        "tf.nn.softmax(\n",
        "    output.logits, axis=1, name=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.3954236e-04, 9.9986041e-01]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAbh_gPMp3E_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4L61nbw6e90"
      },
      "source": [
        "#Masked identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II3a_as36h23",
        "outputId": "87361d60-f6c8-4d4f-ad9f-70434eb1bca9"
      },
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "tokenizer_mask = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model_mask = TFAutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_tf_auto.py:533: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXr9X-b36mnw"
      },
      "source": [
        "sequence = f\"The book was a romantic {tokenizer_mask.mask_token} genre\"\n",
        "input = tokenizer_mask.encode(sequence, return_tensors=\"tf\")\n",
        "mask_token_index = tf.where(input == tokenizer.mask_token_id)[0, 1]\n",
        "token_logits = model_mask(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztELS_527re9",
        "outputId": "6c27452f-350c-4700-b783-3d6d33483b0d"
      },
      "source": [
        "for token in top_5_tokens:\n",
        "     print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The book was a romantic fantasy genre\n",
            "The book was a romantic comedy genre\n",
            "The book was a romantic romance genre\n",
            "The book was a romantic fiction genre\n",
            "The book was a romantic novel genre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FOaiLkg74Vo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}